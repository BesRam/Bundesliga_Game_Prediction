{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f0bba8",
   "metadata": {},
   "source": [
    "# Historical Bundesliga Match Data Scraping and Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ef5ce",
   "metadata": {},
   "source": [
    "Welcome to our Jupyter Notebook focused on scraping historical Bundesliga match data from the past four seasons. In this project, we gather data from https://fbref.com/de/wettbewerbe/20/Bundesliga-Statistiken, which provides comprehensive statistics on Bundesliga matches. Additionally, we incorporate stadium location data from a CSV file obtained from https://github.com/jokecamp/FootballData.git. This extra step is crucial as the original match data lacks specific geographical coordinates, only indicating whether matches were home or away. Our goal is to merge these datasets to enrich our analysis and subsequently store the combined data in a MySQL database for further analysis. This notebook will guide you through the processes of data collection, integration, and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2dbec6-855c-4e40-95bf-61602db55ba3",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbff081-04e0-4028-b3b9-4c6ddf610396",
   "metadata": {},
   "source": [
    "In this section of our notebook, we will establish the foundation necessary for our web scraping and data handling activities. Here, we'll import all the libraries essential for fetching and processing web data, handling data structures, and performing data analysis. These libraries include **'requests'** for sending HTTP requests, **'BeautifulSoup'** from **'bs4'** for parsing HTML content, and **'pandas'** along with **'numpy'** for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for web scraping and data manipulation.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9731cb-2482-4426-9693-b12664eaf51a",
   "metadata": {},
   "source": [
    "We will also define the specific URL that contains the historical data for Bundesliga standings. This URL will serve as our primary data source from which we will scrape the necessary information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26967a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for the Bundesliga standings page, which contains the data we want to scrape.\n",
    "bundesliga_standings_url = 'https://fbref.com/en/comps/20/Bundesliga-Stats'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b1451-88bf-417d-afb2-957a067a82fc",
   "metadata": {},
   "source": [
    "Additionally, we'll execute an initial HTTP GET request to retrieve the HTML content of the page, ensuring we have access to the data needed for our subsequent scraping processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the Bundesliga standings URL to fetch the HTML content of the page.\n",
    "data = requests.get(bundesliga_standings_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb570c7",
   "metadata": {},
   "source": [
    "## 2. Parsing HTML Links with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28b4b8-59db-4d46-8798-48b19c5e5ac7",
   "metadata": {},
   "source": [
    "In this chapter, we will delve into the specifics of extracting data from the HTML content we previously fetched. Using Beautiful Soup, a powerful library for parsing HTML and XML documents, we will navigate the structure of the webpage and isolate the data crucial for our projectâ€”specifically, the links to team pages. This process involves identifying and parsing HTML tables where team data is listed, extracting hyperlink elements, and formatting these links for further use. The steps outlined here are key for efficiently gathering the detailed team-specific data needed for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a BeautifulSoup object with the HTML content to enable parsing.\n",
    "soup = BeautifulSoup(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup's select() method to find the first occurrence of a table with the class 'stats_table'.\n",
    "# This table contains the Bundesliga standings information.\n",
    "standings_table = soup.select('table.stats_table')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all anchor tags (<a>) within the standings table to find relevant hyperlinks.\n",
    "links = standings_table.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0704ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'href' attribute from each anchor tag to gather the individual URLs.\n",
    "links = [l.get(\"href\") for l in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ba6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out URLs to only include those containing '/squads/', which are links to team pages.\n",
    "links = [l for l in links if '/squads/' in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the extracted URLs are relative, prepend the base URL 'https://fbref.com' to each to form complete URLs.\n",
    "team_urls = [f\"https://fbref.com{l}\" for l in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0eca8",
   "metadata": {},
   "source": [
    "## 3. Retrieve Match Stats for Bayer Leverkusen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d6f9b",
   "metadata": {},
   "source": [
    "In this chapter, we will focus on extracting detailed match statistics for Bayer Leverkusen, one of the teams in the Bundesliga. By leveraging the URL we obtained in the previous steps, we can access and scrape the specific team page on fbref.com. This process involves sending a HTTP request to the team's page, retrieving the HTML content, and parsing it to extract data about each game they played."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff3bba",
   "metadata": {},
   "source": [
    "### 3.1 Extract Match Stats using Pandas and Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the first URL from our list, which corresponds to Bayer Leverkusen's page.\n",
    "data = requests.get(team_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aefc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize Pandas to parse the HTML data and extract the table specifically containing \"Scores & Fixtures\".\n",
    "# This table includes detailed statistics for each match played by Bayer Leverkusen.\n",
    "# The read_html function returns a list of DataFrames, where we select the first one that matches our criteria.\n",
    "matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8aca6e",
   "metadata": {},
   "source": [
    "### 3.2 Get Match Shooting Stats with Request and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7c216",
   "metadata": {},
   "source": [
    "We've secured the primary match stats, yet we require a deeper dive into the gameplay details. This deeper analysis can be found on the shooting page, where stats such as total shots, shots on target, and additional metrics like the number of free kicks and penalties are presented. Our first task is to pinpoint the URL of the shooting page, starting from the scores and fixtures page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We parse the HTML content we retrieved using BeautifulSoup.\n",
    "soup = BeautifulSoup(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look for all the 'a' tags to extract hyperlinks from the HTML content.\n",
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a415d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the 'href' attribute from each link to get the actual URLs.\n",
    "links = [l.get(\"href\") for l in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fcf7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter out the list of URLs to keep only the links that contain the reference to 'all_comps/shooting/'.\n",
    "links = [l for l in links if l and 'all_comps/shooting/' in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a04960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making another GET request, this time to the first link of the shooting statistics page.\n",
    "data = requests.get(f\"https://fbref.com{links[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pandas to read the HTML content, specifically looking for the table with 'Shooting' stats.\n",
    "# The function returns a list, so we access the first table with shooting stats using [0].\n",
    "shooting = pd.read_html(data.text, match=\"Shooting\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2af717",
   "metadata": {},
   "source": [
    "### 3.3 Cleaning and Merging Scraped Data With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4cef3",
   "metadata": {},
   "source": [
    "We observe that the DataFrame displays a multi-level index as we look at the first five rows with the head method. However, in this case, the second level of indices doesn't provide much value. Multi-level indices are often unnecessary for our purposes in pandas. We're going to simplify the DataFrame by removing one level of the index. You can recognize the presence of two index levels by the bolded rows, which suggest multiple header rows. After removing an index level, we'll revisit the shooting stats and confirm the adjustment by checking the head of the DataFrame again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df726bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the top level of the multi-index in the shooting DataFrame's columns for clarity.\n",
    "shooting.columns = shooting.columns.droplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9891cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the 'matches' DataFrame with the relevant columns from the 'shooting' DataFrame,\n",
    "# aligning them by the 'Date' column to combine the match and shooting stats.\n",
    "team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb8d54",
   "metadata": {},
   "source": [
    "Following the merge and cleanup process in our notebook, we now have a tidy DataFrame for Bayer Leverkusen's 2023-2024 Bundesliga season. It presents a consolidated view of each match's details along with the shooting statistics, setting us up perfectly for further analysis specific to Bayer Leverkusen's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccd9f5",
   "metadata": {},
   "source": [
    "## 4. Scrapping Data for Multiple Seasons and Teams with a Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c067d012",
   "metadata": {},
   "source": [
    "With our methodology proven successful for Bayer Leverkusen's 2023-2024 season, we're now set to scale up our data scraping process. We'll apply the same procedure to extract and clean data for all teams across the previous four Bundesliga seasons. This comprehensive collection of data will allow us to perform an extensive analysis on the trends and patterns over multiple seasons, enhancing the robustness of our predictive models and insights into the league's dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of years in reverse order, starting from 2024 to 2021, to iterate through past seasons.\n",
    "years = list(range(2024, 2020, -1))\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba17b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store all the match data we will scrape.\n",
    "all_matches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a933d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "standing_url = 'https://fbref.com/en/comps/20/Bundesliga-Stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the time module for pacing our requests to avoid overloading the server.\n",
    "import time\n",
    "\n",
    "# Loop through each year to scrape data season by season.\n",
    "for year in years:\n",
    "    data = requests.get(standing_url)# Fetch the standings page for the current year in the loop.\n",
    "    soup = BeautifulSoup(data.text) # Parse the fetched data using BeautifulSoup.\n",
    "    standings_table = soup.select(\"table.stats_table\")[0] # Select the table that contains the standings.\n",
    "    \n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')] # Extract all links from the standings table.\n",
    "    links = [l for l in links if '/squads/' in l] # Filter out links to team pages specifically.\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links] # Create full URLs from the filtered links.\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "    standing_url = f\"https://fbref.com{previous_season}\"\n",
    "    \n",
    "    # Iterate through each team URL to scrape individual team data.\n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \") # Extract the team name from the URL.\n",
    "        \n",
    "        # Scrape the team's match data.\n",
    "        data = requests.get(team_url)\n",
    "        matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "        \n",
    "        # Parse the team page to extract the shooting data.\n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "        \n",
    "        # Fetch the shooting data page.\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "        shooting.columns = shooting.columns.droplevel() # Drop the top level of multi-index from the columns.\n",
    "        \n",
    "        # Attempt to merge match stats with shooting stats for each team.\n",
    "        # If shooting stats are missing and a ValueError occurs, skip this team and move to the next.\n",
    "        try:\n",
    "            team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\"]], on=\"Date\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        # ilter matches for Bundesliga and include Season and Team columns\n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Bundesliga\"]\n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "\n",
    "        # Append the matches for the team to the list\n",
    "        all_matches.append(team_data)\n",
    "        \n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db09aff",
   "metadata": {},
   "source": [
    "The list all_matches is a collection of dataframes. Each team_data dataframe is added to that list. Now we need to combine all of the individual dataframes into a single dataframe. We can use the concat function in pandas to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data\n",
    "match_df = pd.concat(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column header to lower case\n",
    "match_df.columns = [c.lower() for c in match_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73755708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary column\n",
    "match_df = match_df.drop('match report', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa520b2",
   "metadata": {},
   "source": [
    "## 5. Save Match Data in MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d041ef",
   "metadata": {},
   "source": [
    "After successfully retrieving and merging the match and shooting data for each team, the next critical step is to store this enriched dataset in a MySQL database. Storing the data in MySQL not only secures it for long-term access but also facilitates efficient retrieval for future analysis and reporting. This process involves setting up the database schema, ensuring data integrity, and executing the database commands to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import mysql.connector\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de5638-a0d0-47b6-b7a0-9ace7ae50101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to create a 'matches' table in the database if it doesn't already exist.\n",
    "# Each column is defined with an appropriate data type to ensure data integrity.\n",
    "table_creation_query = \"\"\"\n",
    "         CREATE TABLE IF NOT EXISTS matches (\n",
    "             id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "             date VARCHAR(255),\n",
    "             time VARCHAR(255),\n",
    "             comp VARCHAR(255),\n",
    "             round VARCHAR(255),\n",
    "             day VARCHAR(255),\n",
    "             venue VARCHAR(255),\n",
    "             result VARCHAR(255),\n",
    "             gf FLOAT,\n",
    "             ga FLOAT,\n",
    "             opponent VARCHAR(255),\n",
    "             xg FLOAT,\n",
    "             xga FLOAT,\n",
    "             poss FLOAT,\n",
    "             attendance FLOAT,\n",
    "             captain VARCHAR(255),\n",
    "             formation VARCHAR(255),\n",
    "             referee VARCHAR(255),\n",
    "             notes VARCHAR(255),\n",
    "             sh FLOAT,\n",
    "             sot FLOAT,\n",
    "             dist FLOAT,\n",
    "             fk INT,\n",
    "             pk INT,\n",
    "             pkatt INT,\n",
    "             season INT,\n",
    "             team VARCHAR(255)\n",
    "         );\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb340d2-88cd-48ad-9fa6-417e60dce8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to insert data into each column of the 'matches' table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO matches (\n",
    "    date, time, comp, round, day, venue, result, gf, ga, opponent,\n",
    "    xg, xga, poss, attendance, captain, formation, referee, notes,\n",
    "    sh, sot, dist, fk, pk, pkatt, season, team\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79dda9-8266-43d3-be3f-a39b7d1adbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for insertion by converting the DataFrame row to a list\n",
    "values = list(row[[\n",
    "    'date', 'time', 'comp', 'round', 'day', 'venue', 'result', 'gf', 'ga', 'opponent',\n",
    "    'xg', 'xga', 'poss', 'attendance', 'captain', 'formation', 'referee', 'notes',\n",
    "    'sh', 'sot', 'dist', 'fk', 'pk', 'pkatt', 'season', 'team'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to establish a connection with the MySQL database using the provided credentials.\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        user='root',\n",
    "        password='abcabc123',\n",
    "        host='localhost',\n",
    "        database='adsfootball'\n",
    "    )\n",
    "\n",
    "    # Check if the connection was successful    \n",
    "    if connection.is_connected():\n",
    "        \n",
    "        # Create a cursor object to interact with the database\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Clear any existing table to avoid duplication of structure\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS matches\")\n",
    "\n",
    "        # Execute the SQL query to create a new 'matches' table as defined earlier\n",
    "        cursor.execute(table_creation_query)\n",
    "        \n",
    "        # Convert any NaN values in the DataFrame to None for SQL compatibility\n",
    "        match_df = match_df.fillna(np.nan).replace([np.nan], [None])\n",
    "        \n",
    "        # Insert DataFrame records into the SQL table one row at a time\n",
    "        for i, row in match_df.iterrows():\n",
    "            # Attempt to execute the insert query with the prepared values\n",
    "            try:\n",
    "                cursor.execute(insert_query, values)\n",
    "            except Error as e:\n",
    "                # Log any errors that occur during the insert operation\n",
    "                print(f\"Error inserting data: {e}\")\n",
    "                print(f\"Row with error: {values}\")\n",
    "            \n",
    "        # Commit all changes to the database to ensure data is saved\n",
    "        connection.commit()\n",
    "        print('Data successfully commited.')\n",
    "\n",
    "# Catch and print any errors encountered during database connection or query execution\n",
    "except Error as e:\n",
    "    print(f\"Error while connecting to MySQL: {e}\")\n",
    "    \n",
    "# Ensure the database connection is closed properly\n",
    "finally:\n",
    "    if (connection.is_connected()):\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77b781-a65d-4de5-be31-ba709500bdf8",
   "metadata": {},
   "source": [
    "## 6. Get Stadium Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e293739-36ee-46dc-9d19-356e3dc4a2f7",
   "metadata": {},
   "source": [
    "\n",
    "In our football matches dataset, the specific locations where the games are held are not included. To remedy this, we will utilize a GitHub repository that hosts a CSV file with detailed stadium data, available at: https://github.com/jokecamp/FootballData.git. Our objective is to retrieve this stadium data and integrate it into our MySQL database to enrich our dataset with accurate location information for each match. This step will enhance our analysis capabilities by allowing us to consider the geographical context of the games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebf0a6-e37d-4bda-878a-b802bb60d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing stadium data with GPS coordinates into a Pandas DataFrame.\n",
    "stadium_data_df = pd.read_csv(\"stadiums-with-GPS-coordinates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951f682-02dd-48d1-9eb7-a95b75c62844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only stadiums located in Germany.\n",
    "# This refinement is necessary as our analysis focuses on Bundesliga matches, which are played in Germany.\n",
    "stadium_data_df = stadium_data_df[stadium_data_df[\"Country\"]==\"Germany\"]\n",
    "stadium_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e65a1-c853-4a2a-a0b4-d2de8d851679",
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2686c-d94a-49d6-bbea-eb8caff06f5a",
   "metadata": {},
   "source": [
    "## 7. Save Stadium Data in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9908f78-9b99-45eb-b29f-eb4949e92f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an SQL query to create a new table named 'stadiums' in the database if it does not already exist.\n",
    "table_creation_query_stadium = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stadiums (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    team VARCHAR(255),\n",
    "    fdcouk VARCHAR(255),\n",
    "    city VARCHAR(255),\n",
    "    stadium VARCHAR(255),\n",
    "    capacity INT,\n",
    "    latitude FLOAT,\n",
    "    longitude FLOAT,\n",
    "    country VARCHAR(255)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d00e04-5845-40d7-8d6c-c34bd922f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an SQL insert statement for adding new records into the 'stadiums' table.\n",
    "insert_statement_stadium = \"\"\"\n",
    "INSERT INTO stadiums (team, fdcouk, city, stadium, capacity, latitude, longitude, country)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a76b5e-3bb8-4036-ac1b-073994fe11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to establish a connection to the MySQL database using specified credentials and database information.\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',      \n",
    "        database='adsfootball',  \n",
    "        user='root',           \n",
    "        password='abcabc123'   \n",
    "    )\n",
    "\n",
    "    # Check if the database connection was successful\n",
    "    if connection.is_connected():\n",
    "        # Create a cursor object using the connection\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "         # Drop any existing table named 'stadiums' to prevent duplicate data issues\n",
    "        table_drop_query = \"DROP TABLE IF EXISTS stadiums;\"\n",
    "        cursor.execute(table_drop_query)\n",
    "        \n",
    "        # Create a new 'stadiums' table based on predefined SQL schema\n",
    "        cursor.execute(table_creation_query_stadium)\n",
    "\n",
    "        # Insert data into the 'stadiums' table from the DataFrame row by row\n",
    "        for index, row in stadium_data_df.iterrows():\n",
    "            # Prepare data tuple for insertion\n",
    "            data_tuple = (row['Team'], row['FDCOUK'], row['City'], row['Stadium'], \n",
    "                          row['Capacity'], row['Latitude'], row['Longitude'], row['Country'])\n",
    "            cursor.execute(insert_statement_stadium, data_tuple)\n",
    "        \n",
    "        # Commit all changes to the database to ensure data is saved\n",
    "        connection.commit()\n",
    "        print('Data successfully commited.')\n",
    "        \n",
    "        # Close the cursor to release database resources\n",
    "        cursor.close()\n",
    "        \n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the database connection is closed properly\n",
    "    if connection and connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
