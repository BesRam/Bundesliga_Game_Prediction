{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e264a3-d034-40c1-b95f-698b06b30fd3",
   "metadata": {},
   "source": [
    "# Retrieving Historical Weather Data for Bundesliga Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d2b8a-41e2-40c9-9248-bb375c2e6b70",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, our goal is to retrieve historical weather data to correlate with football matches. Building upon our prior work in the 'Football_Bundesliga_WebScraping.ipynb' notebook, we have already extracted and stored stadium data in a MySQL database. This data includes the locations and dates of each football match, providing us with the precise venues for these events. With this foundational information at hand, our next step in this notebook is to use the OpenMeteo Historical Weather API to fetch the historical weather conditions for each match location at the time it was played. Integrating accurate weather data into our dataset is crucial for a comprehensive analysis, as it may significantly influence match outcomes. After successfully retrieving and verifying the weather data, we will store it in our MySQL database. This approach not only keeps our data well-organized but also facilitates accessibility for further analysis and model development, allowing for more nuanced predictions and insights into how weather conditions could have impacted past football games.\n",
    "\n",
    "We retrieve weather data from this source: [OpenMeteo Historical Weather API](https://open-meteo.com/en/docs/historical-weather-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0fd47-a90f-4d24-b2f4-ba791e4d0eb9",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a2bf3-ba8d-4c9c-b938-14d59cccc14d",
   "metadata": {},
   "source": [
    "In this initial chapter, we prepare our Jupyter Notebook by importing necessary libraries and modules that enable data manipulation, database connectivity, and API requests. This setup ensures that we have all the tools required to retrieve and handle weather data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3db109-41a9-43c0-8b48-ffec0756383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import openmeteo_requests # Custom module for managing requests to the OpenMeteo API.\n",
    "import requests_cache # Caches the responses of HTTP requests to enhance efficiency and reduce load times.\n",
    "from retry_requests import retry # Provides a mechanism to automatically retry HTTP requests on failure.\n",
    "import time # Allows us to use functionality related to time, such as delays and timestamp calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa96233-b9d4-41da-83c2-bc602a348d07",
   "metadata": {},
   "source": [
    "## 2. Get Football Matches and Stadium Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ade96e-4265-4036-b820-9d5f18055ff4",
   "metadata": {},
   "source": [
    "In this chapter, we focus on retrieving data from our MySQL database. This includes the football matches and associated stadium information previously stored. This data is crucial as it forms the basis for our subsequent weather data integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af32e72-677c-4c95-867e-ff01ac290c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Establish a connection to the MySQL database using specified credentials.\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        database='adsfootball',\n",
    "        user='root',\n",
    "        password='abcabc123'\n",
    "    )\n",
    "    \n",
    "    # SQL query to retrieve all entries from the 'matches' table.\n",
    "    query = \"SELECT * FROM matches\"\n",
    "    \n",
    "    # Execute the SQL query and load the resulting data directly into a pandas DataFrame.\n",
    "    football_matches_df = pd.read_sql_query(query, connection)\n",
    "    \n",
    "    # Output the first few rows of the DataFrame to verify the data has been loaded correctly.\n",
    "    print(football_matches_df.head())\n",
    "    \n",
    "except Error as e:\n",
    "    # If an error occurs during the database operations, print the error message.\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Check if the database connection is still open and close it to free resources.\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96bf4e7-237e-43ab-8d7b-588b7f2975de",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Establish a connection to the MySQL database using the provided credentials.\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        database='adsfootball',\n",
    "        user='root',\n",
    "        password='abcabc123'\n",
    "    )\n",
    "    \n",
    "    # Prepare an SQL query to fetch all records from the 'stadiums' table.\n",
    "    query = \"SELECT * FROM stadiums\"\n",
    "    \n",
    "    # Execute the SQL query and store the results in a DataFrame for easy data manipulation.\n",
    "    stadium_data_df = pd.read_sql_query(query, connection)\n",
    "    \n",
    "    # Display the first few rows of the DataFrame to check the data and ensure it's loaded correctly.\n",
    "    print(stadium_data_df.head())\n",
    "    \n",
    "except Error as e:\n",
    "    # Output any errors encountered during the database operation.\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure that the database connection is closed to release system resources.\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb04c7-5967-4253-8d27-79d47b4304b3",
   "metadata": {},
   "source": [
    "In our football matches dataset, specific details about match locations, such as coordinates or names, are not directly provided. Instead, the dataset includes a column labeled 'venue' that specifies whether the match was played at 'Home' or 'Away.' This implies that when the 'venue' is marked as 'Home,' the match location corresponds to the home stadium of the team listed in the 'team' column. For instance, if Bayer Leverkusen is listed as the 'team' and the 'venue' is 'Home,' the game was held at Bayer Leverkusen’s stadium. However, for accurate retrieval of weather data, precise geographic coordinates are essential. Thus, we need to merge the football matches dataset with the stadium dataset, which provides detailed information such as team names, stadium names, and the exact coordinates of each stadium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d10eb6-dfc5-4510-b8d3-34f096319ec8",
   "metadata": {},
   "source": [
    "## 3. Organize Football Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6badc-9673-4d95-b67c-feb81ece3baa",
   "metadata": {},
   "source": [
    "In this chapter, we aim to merge data based on team names, but we face a challenge due to inconsistencies in team naming across the 'team' and 'opponent' columns. For example, 'Eintracht Frankfurt' is shortened to 'Eint Frankfurt' in the 'opponent' column, and 'Bayer Leverkusen' becomes 'Leverkusen.' To ensure accurate merging, we need to standardize the team names in both columns. This is critical because our merging logic depends on the 'team' column to determine the game location based on whether the 'venue' is 'Home' or 'Away'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cbae8-9ef7-4185-9b18-3bcf853feafa",
   "metadata": {},
   "source": [
    "This function identifies discrepancies in team naming between the 'team' and 'opponent' columns of the dataset. It collects unique team names from both columns, compares them to find differences, and returns a set of team names that are referred to differently across the two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62a367-8bf7-4fdf-a657-31ef1b64b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify inconsistencies in team naming between 'team' and 'opponent' columns\n",
    "def find_teams_with_different_names(df):\n",
    "    # Collect unique team names from both 'team' and 'opponent' columns into separate sets\n",
    "    team_names = set(df['team'])\n",
    "    opponent_names = set(df['opponent'])\n",
    "    \n",
    "    # Determine names present in one column but not the other\n",
    "    diff_in_team = team_names.difference(opponent_names)\n",
    "    diff_in_opponent = opponent_names.difference(team_names)\n",
    "    \n",
    "    # Combine unique names from both comparisons into a single set\n",
    "    all_differences = diff_in_team.union(diff_in_opponent)\n",
    "    \n",
    "    return all_differences\n",
    "\n",
    "# Execute the function and store the results\n",
    "different_names = find_teams_with_different_names(football_matches_df)\n",
    "\n",
    "# Print the team names that differ between columns\n",
    "print(\"Teams with different names by column:\", different_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d334d-184b-460d-9b2d-b48c921f46a4",
   "metadata": {},
   "source": [
    "As we can see, the following teams are named differently in both columns ('**team**' and 'opponent'):\n",
    "- **Bayer Leverkusen** (Leverkusen)\n",
    "- **Monchengladbach** (M'Gladbach)\n",
    "- **Greuther Furth** (Greuter Fürth)\n",
    "- **Eintracht Frankfurt** (Eint Frankfurt)\n",
    "- **Koln** (Köln)\n",
    "\n",
    "We want to standardize them across both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59264a5-6f48-41a7-a296-a6c9b7beab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary where keys are alternative names and values are the standard names\n",
    "name_mapping = {\n",
    "    'Leverkusen': 'Bayer Leverkusen',\n",
    "    'Eint Frankfurt': 'Eintracht Frankfurt',\n",
    "    'M\\'Gladbach': 'Monchengladbach',\n",
    "    'Köln': 'Koln',\n",
    "    'Greuther Fürth': 'Greuther Furth'\n",
    "}\n",
    "\n",
    "# Define a function to standardize team names based on a provided mapping dictionary\n",
    "def standardize_team_names(df, mapping):\n",
    "    # Update 'opponent' column in the DataFrame using the mapping to ensure uniformity\n",
    "    df['opponent'] = df['opponent'].apply(lambda x: mapping.get(x, x))\n",
    "    return df\n",
    "\n",
    "# Apply the function to standardize the names in the 'opponent' column of the DataFrame\n",
    "football_matches_df = standardize_team_names(football_matches_df, name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873e50d-67cd-4f51-93e7-2b47e23e4c9a",
   "metadata": {},
   "source": [
    "Now we can check again for differences in the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c39d7-d0fc-4d33-8c76-03e013c3cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find teams with different names in the dataset\n",
    "def find_teams_with_different_names(df):\n",
    "    # Collect 'team' and 'opponent' names into two separate sets\n",
    "    team_names = set(df['team'])\n",
    "    opponent_names = set(df['opponent'])\n",
    "    \n",
    "    # Find the differences in names between both columns\n",
    "    diff_in_team = team_names.difference(opponent_names)\n",
    "    diff_in_opponent = opponent_names.difference(team_names)\n",
    "    \n",
    "    # Combine the differences from both sets\n",
    "    all_differences = diff_in_team.union(diff_in_opponent)\n",
    "    \n",
    "    return all_differences\n",
    "\n",
    "# Calling the function\n",
    "different_names = find_teams_with_different_names(football_matches_df)\n",
    "print(\"Teams with different names by column:\", different_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6256ef-32cd-43d7-a329-52f22813d72f",
   "metadata": {},
   "source": [
    "Another inconsistency we encounter is that the team names in the stadium dataset are written differently compared to those in the football matches dataset. To ensure accurate data integration, we need to standardize these names by addressing variations such as 'Umlauts' (e.g., converting 'ü' to 'u') and replacing or removing certain prefixes and suffixes. This standardization is crucial for maintaining consistency across our datasets and enabling effective data merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958ba01-8e0a-4458-b5b0-e6f34aa8a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace umlauts and other special characters\n",
    "def replace_umlauts(name):\n",
    "    umlaut_replacements = {'ä': 'a', 'ö': 'o', 'ü': 'u', 'ß': 'ss'}\n",
    "    for umlaut, replacement in umlaut_replacements.items():\n",
    "        name = name.replace(umlaut, replacement)\n",
    "    return name\n",
    "\n",
    "# Function to normalize team names\n",
    "def normalize_name(name):\n",
    "    name = replace_umlauts(name)\n",
    "    # Remove prefixes, spaces, and other characters for standardization\n",
    "    return name.lower().replace('1.', '').replace('fc', '').replace(' ', '').replace('.', '').replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc9e1a-7b3f-4ef6-94ca-ff13affa5f43",
   "metadata": {},
   "source": [
    "We now want to check if there are any teams that are listed in the football matches dataset but are missing in the stadium dataset. This verification is essential to ensure data completeness and consistency across both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e1f34-64e7-469c-b3d4-6b28f29b6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all unique teams in football_matches_df\n",
    "all_teams = pd.concat([football_matches_df['team'], football_matches_df['opponent']]).unique()\n",
    "all_teams_normalized = [normalize_name(team) for team in all_teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a34b5-8878-41f0-b63e-548fc1dabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of normalized team names in stadium_data_df\n",
    "stadium_teams_normalized = [normalize_name(team) for team in stadium_data_df['fdcouk']]\n",
    "stadium_teams_normalized.extend([normalize_name(team) for team in stadium_data_df['team']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5bcff-e777-4e81-9dbb-a520c0a47b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find teams in football_matches_df that have no match in stadium_data_df\n",
    "missing_teams = set()\n",
    "for team in all_teams_normalized:\n",
    "    if not any(team in stadium_team for stadium_team in stadium_teams_normalized):\n",
    "        missing_teams.add(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2660b-2d72-4981-88d6-9e891509a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the teams that have no match\n",
    "print(\"Teams in football_matches_df with no match in stadium_data_df:\")\n",
    "for team in missing_teams:\n",
    "    print(team)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9c212-67b3-43df-a2c5-bd16ecf98699",
   "metadata": {},
   "source": [
    "Here we can see that following teams are missing in the stadium dataset:\n",
    "- VfL Bochum\n",
    "- FC Heidenheim\n",
    "- FC Union Berlin\n",
    "- Arminia Bielefeld\n",
    "- SV Darmstadt 98\n",
    "- RB Leipzig\n",
    "\n",
    "We have to add them manually in the dataset with the corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b43fb4-acb0-44b4-a19b-2f7963c5473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new teams to be added to the stadium_data_df\n",
    "new_teams = [\n",
    "    # Dictionary for each new team\n",
    "    # Each dictionary contains all the required information for the team\n",
    "    {\"id\": 24, \"team\": \"1. FC Heidenheim\", \"fdcouk\": \"Heidenheim\", \"city\": \"Heidenheim\", \"stadium\": \"Voith-Arena\", \"capacity\": 15000, \"latitude\": 48.2230555556, \"longitude\": 9.02666666667, \"country\": \"Germany\"},\n",
    "    {\"id\": 25, \"team\": \"1. FC Union Berlin\", \"fdcouk\": \"Union Berlin\", \"city\": \"Berlin\", \"stadium\": \"Stadion An der Alten Försterei\", \"capacity\": 22012, \"latitude\": 52.454331516, \"longitude\": 13.56749773, \"country\": \"Germany\"},\n",
    "    {\"id\": 26, \"team\": \"RB Leipzig\", \"fdcouk\": \"RB Leipzig\", \"city\": \"Leipzig\", \"stadium\": \"Red Bull Arena\", \"capacity\": 41122, \"latitude\": 51.3408086368, \"longitude\": 12.3422636309, \"country\": \"Germany\"},\n",
    "    {\"id\": 27, \"team\": \"Arminia Bielefeld\", \"fdcouk\": \"Arminia Bielefeld\", \"city\": \"Bielefeld\", \"stadium\": \"SchücoArena\", \"capacity\": 27332, \"latitude\": 52.031389, \"longitude\": 8.516944, \"country\": \"Germany\"},\n",
    "    {\"id\": 28, \"team\": \"SV Darmstadt 98\", \"fdcouk\": \"Darmstadt 98\", \"city\": \"Darmstadt\", \"stadium\": \"Merck-Stadion am Böllenfalltor\", \"capacity\": 17810, \"latitude\": 49.854663248, \"longitude\": 8.66999732, \"country\": \"Germany\"},\n",
    "    {\"id\": 29, \"team\": \"VfL Bochum\", \"fdcouk\": \"Bochum\", \"city\": \"Bochum\", \"stadium\": \"Vonovia Ruhrstadion\", \"capacity\": 26000, \"latitude\": 51.4872597176, \"longitude\": 7.23525905896, \"country\": \"Germany\"}\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "new_teams_df = pd.DataFrame(new_teams)\n",
    "\n",
    "# Append the new teams DataFrame to the existing stadium_data_df\n",
    "stadium_data_df = pd.concat([stadium_data_df, new_teams_df], ignore_index=True)\n",
    "stadium_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a72fa-26da-453b-b556-2c5743c917ad",
   "metadata": {},
   "source": [
    "## 3. Match Football Data with Stadium Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c71c16-c012-422c-8f14-571751b83bb9",
   "metadata": {},
   "source": [
    "In this chapter, we will merge our organized football match data with stadium data. This integration allows us to map each match to its corresponding stadium, ensuring we have accurate location information for subsequent weather data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55786313-fa29-4a44-948d-91dd90f8bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match team names between the football matches dataset and the stadium dataset\n",
    "def match_team_names(football_name, stadium_df):\n",
    "    # Normalize the football team name for comparison\n",
    "    football_name_norm = normalize_name(football_name)\n",
    "    best_match = None\n",
    "    shortest_length = float('inf')  # Initialize with infinity to find the minimum length string match\n",
    "\n",
    "    # Iterate through each row in the stadium dataset\n",
    "    for _, row in stadium_df.iterrows():\n",
    "        # Check against both 'fdcouk' and 'team' columns in the stadium dataset\n",
    "        for column in ['fdcouk', 'team']:\n",
    "            stadium_name_norm = normalize_name(row[column])\n",
    "            # Check if one normalized name contains the other\n",
    "            if football_name_norm in stadium_name_norm or stadium_name_norm in football_name_norm:\n",
    "                # Update the best match if the current name is shorter than previously found names\n",
    "                if len(stadium_name_norm) < shortest_length:\n",
    "                    shortest_length = len(stadium_name_norm)\n",
    "                    best_match = row['fdcouk']\n",
    "    \n",
    "    return best_match\n",
    "\n",
    "# This function ensures that we find the closest match based on the shortest normalized name that includes the football name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2adab-6d09-4331-b01b-b711a3caec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced function to retrieve coordinates, city, and stadium information for a given team\n",
    "def get_stadium_info(football_team, venue, opponent, stadium_df):\n",
    "    # Determine which team to search for based on the venue\n",
    "    team_to_search = football_team if venue == 'Home' else opponent\n",
    "    # Find the best matching stadium team name\n",
    "    matched_stadium_team = match_team_names(team_to_search, stadium_df)\n",
    "    \n",
    "    # If a match is found, retrieve additional information from the stadium dataset\n",
    "    if matched_stadium_team:\n",
    "        team_row = stadium_df[stadium_df['fdcouk'] == matched_stadium_team]\n",
    "        if not team_row.empty:\n",
    "            # Return the latitude, longitude, city, and stadium name\n",
    "            return (team_row.iloc[0]['latitude'], team_row.iloc[0]['longitude'],\n",
    "                    team_row.iloc[0]['city'], team_row.iloc[0]['stadium'])\n",
    "    \n",
    "    # Return None for all fields if no match is found\n",
    "    return (None, None, None, None)\n",
    "\n",
    "# This function is crucial for providing a comprehensive set of stadium data for each match,\n",
    "# enhancing the analysis capabilities with precise location details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a28aee-6892-4ac4-994c-ffa474f33492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to football_matches_df\n",
    "# Unpack the tuple directly into the new columns\n",
    "football_matches_df[['latitude', 'longitude', 'city', 'stadium']] = pd.DataFrame(\n",
    "    football_matches_df.apply(\n",
    "        lambda x: get_stadium_info(x['team'], x['venue'], x['opponent'], stadium_data_df), \n",
    "        axis=1\n",
    "    ).tolist(), index=football_matches_df.index\n",
    ")\n",
    "\n",
    "# Output the updated DataFrame to check the newly added data\n",
    "football_matches_df.head()\n",
    "\n",
    "# This code block effectively adds geographical and venue-related details to the football matches DataFrame.\n",
    "# It enriches each match entry with precise location data including latitude, longitude, city, and stadium name by utilizing the 'get_stadium_info' function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67fb76-d76b-4ac1-9546-baabce1ecee5",
   "metadata": {},
   "source": [
    "## 4. Get Weather Data for Every Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6baa80e-0da6-48f2-9bff-1a5551e5d727",
   "metadata": {},
   "source": [
    "In this chapter, we transition from organizing match data to collecting historical weather data for each game's date and location. Utilizing the OpenMeteo Historical Weather API, we will retrieve weather conditions that are crucial for comprehensive match analysis. Instead of descriptive weather categories, the API provides weather codes based on the World Meteorological Organization (WMO) standards, which can be referenced using the provided [WMO Code Table](https://www.nodc.noaa.gov/archive/arc0021/0002199/1.1/data/0-data/HTML/WMO-CODE/WMO4677.HTM). Upon gathering this information, we will store the weather data in our MySQL database for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab084005-5c4b-46ce-b2e4-fc62a471cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a cached session for HTTP requests to reduce load times and API calls. Cache does not expire.\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f91ff-8a25-4d28-b5ae-a95750e0f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the cached session in a retry mechanism, configuring it to retry up to 5 times with a backoff factor of 0.2 seconds.\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb53a52-d984-4b3a-8b0d-011fcd4157c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenMeteo API client with the retry-enabled cached session.\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac848f5-d9a9-48ac-b105-ba26d336a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL for the Open-Meteo API's archive endpoint.\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6c765-5728-482a-b9e4-fa8f25e8d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with specified columns to store the weather data retrieved from the API.\n",
    "weather_df = pd.DataFrame(columns=['date', 'latitude', 'longitude', 'weather_code', 'mean_temperature', 'precipitation_sum', 'rain_sum', 'snowfall_sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d455c7-6766-42ee-a6dc-38e8a7289730",
   "metadata": {},
   "source": [
    "Open-Meteo's free version permits up to 600 API calls per minute. Given our requirement to make approximately 2500 calls, as dictated by the number of entries in our football matches dataset, we need to manage our requests carefully. Once we reach the limit of 599 calls, we will pause our requests for just over a minute before continuing, ensuring we do not exceed the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a7f69-e687-4db0-ae43-ce4a20c73c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the request count to keep track of how many API calls have been made.\n",
    "request_count = 0\n",
    "# Set the maximum number of requests we can make before needing to pause.\n",
    "max_requests_before_pause = 599\n",
    "# Define the duration in seconds for which to pause once the limit is reached to avoid exceeding the API's rate limit.\n",
    "pause_duration = 61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c0e79-cd02-4d36-a831-c6a59e0d3e05",
   "metadata": {},
   "source": [
    "The parameters we want to obtain are:\n",
    "- **Weather Code** (WMO-Codes)\n",
    "- **Mean Temperature** (°C)\n",
    "- **Total Precipitation** (mm)\n",
    "- **Total Rainfall** (mm)\n",
    "- **Total Snowfall** (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a292178-de10-43d8-9661-21ef1143b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the football matches DataFrame\n",
    "for index, row in football_matches_df.iterrows():\n",
    "    # Before making an API call, check if the request limit has been reached\n",
    "    if request_count >= max_requests_before_pause:\n",
    "        # Notify and pause execution to adhere to API rate limits\n",
    "        print(f\"Maximum number of requests reached ({max_requests_before_pause}). Pausing for {pause_duration} seconds.\")\n",
    "        # Pause execution for the specified duration\n",
    "        time.sleep(pause_duration)\n",
    "        # Reset request count after pausing\n",
    "        request_count = 0\n",
    "\n",
    "    # Extract match date, and geographic coordinates from the DataFrame row\n",
    "    match_date = row['date']\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "\n",
    "    # Configure the parameters for the weather API request\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": match_date,\n",
    "        \"end_date\": match_date,\n",
    "        \"daily\": [\"weather_code\", \"temperature_2m_mean\", \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\"],\n",
    "        \"timezone\": \"Europe/Berlin\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Perform the API call to retrieve weather data\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        daily = responses[0].Daily()\n",
    "        \n",
    "        # Extract weather data from the response\n",
    "        weather_data = daily.Variables(0).ValuesAsNumpy()\n",
    "        mean_temperature = daily.Variables(1).ValuesAsNumpy()\n",
    "        precipitation_sum = daily.Variables(2).ValuesAsNumpy()\n",
    "        rain_sum = daily.Variables(3).ValuesAsNumpy()\n",
    "        snowfall_sum = daily.Variables(4).ValuesAsNumpy()\n",
    "        \n",
    "        # Ensure all data is non-null before appending to the DataFrame\n",
    "        if pd.notna(weather_data).all():\n",
    "            # Create a new DataFrame row with the extracted weather data\n",
    "            new_row = pd.DataFrame({\n",
    "                'date': [match_date], \n",
    "                'latitude': [latitude], \n",
    "                'longitude': [longitude], \n",
    "                'weather_code': [weather_data], \n",
    "                'mean_temperature':[mean_temperature], \n",
    "                'precipitation_sum':[precipitation_sum], \n",
    "                'rain_sum':[rain_sum], \n",
    "                'snowfall_sum':[snowfall_sum]\n",
    "            })\n",
    "            # Append the new row to the main weather DataFrame\n",
    "            weather_df = pd.concat([weather_df, new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Print an error message if the API call fails\n",
    "        print(f\"Error retrieving weather for {row['date']}: {str(e)}\")\n",
    "\n",
    "# After all data is collected, convert certain columns to float for consistent data types\n",
    "weather_df['weather_code'] = weather_df['weather_code'].astype(float)\n",
    "weather_df['mean_temperature'] = weather_df['mean_temperature'].astype(float)\n",
    "weather_df['precipitation_sum'] = weather_df['precipitation_sum'].astype(float)\n",
    "weather_df['rain_sum'] = weather_df['rain_sum'].astype(float)\n",
    "weather_df['snowfall_sum'] = weather_df['snowfall_sum'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288059bc-fb73-402a-8fe7-62f1a608f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de5201-3123-494f-ab68-81b17b66346a",
   "metadata": {},
   "source": [
    "## 5. Store Weather Data in MySQL Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa355cf5-1cf6-4084-9b83-cda3f310c81f",
   "metadata": {},
   "source": [
    "In this chapter, we will focus on securely storing the retrieved weather data into a MySQL database. This process ensures that the weather data is integrated and archived in a structured format, facilitating easy access and analysis alongside the football match data. By the end of this section, the weather data will be systematically stored, ready for further analysis and integration with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b48114-ebcf-45e2-a317-641c6c710964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to create a new table for storing weather data, if it doesn't already exist\n",
    "create_table_query_weather = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS weather (\n",
    "                id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                date VARCHAR(255),\n",
    "                latitude FLOAT,\n",
    "                longitude FLOAT,\n",
    "                weather_code FLOAT,\n",
    "                mean_temperature FLOAT,\n",
    "                precipitation_sum FLOAT,\n",
    "                rain_sum FLOAT,\n",
    "                snowfall_sum FLOAT\n",
    "            );\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3a2c7-138f-46b8-b050-70439e3f1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query template for inserting weather data into the weather table\n",
    "insert_query_weather = \"\"\"\n",
    "INSERT INTO weather (date, latitude, longitude, weather_code, mean_temperature, precipitation_sum, rain_sum, snowfall_sum)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce2c3a-cbae-4ec1-bc7e-302a2963cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the MySQL database\n",
    "try:\n",
    "    connection = mysql.connector.connect(user='root', password='abcabc123', host='localhost', database='adsfootball')\n",
    "    if connection.is_connected():\n",
    "        # Create a cursor object to interact with the database\n",
    "        cursor = connection.cursor()\n",
    "        # Ensure there is no previous 'weather' table that might conflict with the new data\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS weather\")\n",
    "        # Execute the table creation query\n",
    "        cursor.execute(create_table_query_weather)\n",
    "\n",
    "        # Insert each row of weather data from the DataFrame into the MySQL table\n",
    "        for i, row in weather_df.iterrows():\n",
    "            values = (row['date'], row['latitude'], row['longitude'], row['weather_code'], row['mean_temperature'], row['precipitation_sum'], row['rain_sum'], row['snowfall_sum'])\n",
    "            cursor.execute(insert_query_weather, values)\n",
    "\n",
    "        # Commit the transaction to make sure all data is saved in the database\n",
    "        connection.commit()\n",
    "        print(\"All data successfully committed.\")\n",
    "\n",
    "except Error as e:\n",
    "    # Handle any errors that occur during the database connection or execution\n",
    "    print(f\"Error while connecting to MySQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the database connection to release resources\n",
    "    if connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e84ec4-d531-4b81-8a4f-f8a198ba9300",
   "metadata": {},
   "source": [
    "## 6. Update Football Matches Table in MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3075233-1a4b-498e-b086-f097308d69e3",
   "metadata": {},
   "source": [
    "In this chapter, we address the inconsistencies and updates required for the football matches table in our MySQL database. After standardizing team names and enhancing our dataset with additional location details such as coordinates and stadium information, we need to update our database to reflect these changes. This process involves checking for existing columns, dropping them if necessary, adding new columns, and updating each record with precise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db7f57-5f4d-4a6f-a3fa-dab9839c5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for MySQL database connection\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'adsfootball',\n",
    "    'user': 'root',\n",
    "    'password': 'abcabc123'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f6a0c-801d-40ad-bd0f-25e247c6eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if there are existing columns in the table\n",
    "def check_and_drop_columns(cursor, table_name):\n",
    "    # Execute a query to fetch all column names from the specified table\n",
    "    cursor.execute(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "    columns = [column[0] for column in cursor.fetchall()]\n",
    "\n",
    "    # List of columns to be potentially dropped\n",
    "    columns_to_drop = ['latitude', 'longitude', 'city', 'stadium']\n",
    "\n",
    "    # Loop through the list and drop each column if it exists in the table\n",
    "    for column in columns_to_drop:\n",
    "        if column in columns:\n",
    "            try:\n",
    "                cursor.execute(f\"ALTER TABLE {table_name} DROP COLUMN {column}\")\n",
    "            except mysql.connector.Error as err:\n",
    "                print(f\"Failed to drop column {column}: {err.msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681b032-4f1c-4133-ba65-45de26ef28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add the new columns to the table\n",
    "def add_columns(cursor):\n",
    "    # SQL statements to add new columns to the matches table\n",
    "    add_column_statements = [\n",
    "        \"ALTER TABLE matches ADD COLUMN latitude DECIMAL(10, 8)\",\n",
    "        \"ALTER TABLE matches ADD COLUMN longitude DECIMAL(11, 8)\",\n",
    "        \"ALTER TABLE matches ADD COLUMN city VARCHAR(255)\",\n",
    "        \"ALTER TABLE matches ADD COLUMN stadium VARCHAR(255)\"\n",
    "    ]\n",
    "    # Execute each statement and handle errors such as column already existing\n",
    "    for statement in add_column_statements:\n",
    "        try:\n",
    "            cursor.execute(statement)\n",
    "        except mysql.connector.Error as err:\n",
    "            if err.errno == 1060:  # Handle 'Column already exists' error\n",
    "                print(f\"Column already exists: {err.msg}\")\n",
    "            else:\n",
    "                print(f\"An error occurred: {err.msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d68913-17d7-4064-804b-9533a030d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement to update location and stadium details in the matches table\n",
    "update_statement_matches = \"\"\"\n",
    "UPDATE matches\n",
    "SET latitude = %s, longitude = %s, city = %s, stadium = %s\n",
    "WHERE id = %s;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350adc12-8580-44cd-87d9-bc03fe13c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection using the database configuration and handle the database operations\n",
    "try:\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Remove any redundant columns from the table\n",
    "        check_and_drop_columns(cursor, \"matches\")\n",
    "        \n",
    "        # Add new columns to store additional location data\n",
    "        add_columns(cursor)\n",
    "        \n",
    "        # Update each row with new geographic and stadium information\n",
    "        for index, row in football_matches_df.iterrows():\n",
    "            data = (row['latitude'], row['longitude'], row['city'], row['stadium'], row['id'])\n",
    "            cursor.execute(update_statement_matches, data)\n",
    "        \n",
    "        # Commit all changes to ensure they are saved in the database\n",
    "        connection.commit()\n",
    "        print(\"Data has been successfully updated.\")\n",
    "        \n",
    "except Error as e:\n",
    "    # Handle any errors that occur during the database operations\n",
    "    print(f\"Error connecting to the MySQL database: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the database connection is closed after operations complete\n",
    "    if connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806d700-08ce-4f66-8433-1b22ecbe4a6c",
   "metadata": {},
   "source": [
    "**We must also update the 'opponent' column to ensure that team names are correctly written.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ed2fc-6269-4860-ab15-dea41190d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement to update the 'opponent' names in the matches table\n",
    "update_opponent_statement = \"\"\"\n",
    "UPDATE matches\n",
    "SET opponent = %s\n",
    "WHERE id = %s;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a73bf-5d17-4f27-815d-5cff05e965ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the database using the predefined configuration\n",
    "try:\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "\n",
    "    # Check if the database connection was successful\n",
    "    if connection.is_connected():\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Iterate through the DataFrame and update the 'opponent' column with corrected names\n",
    "        for index, row in football_matches_df.iterrows():\n",
    "            # Data tuple containing new opponent name and the corresponding match ID\n",
    "            opponent_data = (row['opponent'], row['id'])\n",
    "            cursor.execute(update_opponent_statement, opponent_data)\n",
    "        \n",
    "        # Commit the updates to the database to ensure all changes are saved\n",
    "        connection.commit()\n",
    "        print(\"Data has been successfully updated.\")\n",
    "\n",
    "except Error as e:\n",
    "    # Handle any errors encountered during the connection or update process\n",
    "    print(f\"Error connecting to the MySQL database: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the database connection and cursor to free resources\n",
    "    if connection.is_connected():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaba71-12ae-44fd-aaaf-804ba5e29c0f",
   "metadata": {},
   "source": [
    "## 7. Update Stadium Table in MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f6ba6-ac3a-4529-bb6b-5e4d0a2c4377",
   "metadata": {},
   "source": [
    "In this final chapter, we focus on refining our stadium dataset stored in MySQL. Our goal is to enhance the dataset by incorporating any missing stadiums. This update ensures that our database reflects the most accurate and comprehensive stadium information available, supporting precise location-based analyses for future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8b7f3-9855-4d75-b5a8-77aec3f22182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to drop the existing 'stadiums' table if it exists, ensuring a fresh start\n",
    "table_drop_query_stadiums = \"DROP TABLE IF EXISTS stadiums;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83de4a9-6d7d-49e0-8c01-53a93197ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to create a new 'stadiums' table with columns for team, stadium, and location details\n",
    "table_creation_query_stadiums = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stadiums (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            team VARCHAR(255),\n",
    "            fdcouk VARCHAR(255),\n",
    "            city VARCHAR(255),\n",
    "            stadium VARCHAR(255),\n",
    "            capacity INT,\n",
    "            latitude FLOAT,\n",
    "            longitude FLOAT,\n",
    "            country VARCHAR(255)\n",
    "        );\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80b8a6-3236-473c-b348-0990b672c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement for inserting data into the 'stadiums' table\n",
    "insert_statement_stadiums = (\n",
    "            \"INSERT INTO stadiums (team, fdcouk, city, stadium, capacity, latitude, longitude, country) \"\n",
    "            \"VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb9f25-c31a-4da7-ac7e-94fee7ec98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the MySQL database\n",
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        database='adsfootball',\n",
    "        user='root',\n",
    "        password='abcabc123'\n",
    "    )\n",
    "\n",
    "    # Check if the database connection was successful\n",
    "    if connection.is_connected():\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Execute the query to drop the existing 'stadiums' table\n",
    "        cursor.execute(table_drop_query_stadiums)\n",
    "        \n",
    "        # Create a new 'stadiums' table based on the defined schema\n",
    "        cursor.execute(table_creation_query_stadiums)\n",
    "        \n",
    "        # Iterate over each row in the stadium DataFrame to insert data\n",
    "        for index, row in stadium_data_df.iterrows():\n",
    "            data_tuple = (row['team'], row['fdcouk'], row['city'], row['stadium'], row['capacity'], \n",
    "                          row['latitude'], row['longitude'], row['country'])\n",
    "            cursor.execute(insert_statement_stadiums, data_tuple)\n",
    "        \n",
    "        # Commit the transaction to save all changes\n",
    "        connection.commit()\n",
    "        print(\"Records inserted.\")\n",
    "        \n",
    "        # Handle any errors that occur during the database operations\n",
    "        cursor.close()\n",
    "        \n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the database connection is closed to free resources\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
